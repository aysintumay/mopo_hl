env:
  task: halfcheetah-random-v0
  transition_params:
    model_batch_size: 2048
    use_weight_decay: true
    optimizer_class: Adam
    learning_rate: 0.001
    holdout_ratio: 0.2
    inc_var_loss: true
    model:
      hidden_dims:
      - 200
      - 200
      - 200
      - 200
      decay_weights:
      - 2.5e-05
      - 5.0e-05
      - 7.5e-05
      - 7.5e-05
      - 0.0001
      act_fn: swish
      out_act_fn: identity
      num_elite: 5
      ensemble_size: 7
  mopo_params:
    max_epoch: 400
    rollout_batch_size: 50000
    rollout_mini_batch_size: 10000
    model_retain_epochs: 1
    num_env_steps_per_epoch: 1000
    train_model_interval: 250
    max_trajectory_length: 1000
    eval_interval: 1000
    num_eval_trajectories: 10
    snapshot_interval: 2000
    model_env_ratio: 0.95
    max_model_update_epochs_to_improve: 5
    max_model_train_iterations: None
train:
  epoch: 100
  step_per_epoch: 1000
  eval_episodes: 10
  batch_size: 256
  log_freq: 1000
  terminal_counter: 1
model:
  _target_: src.models.transition_model.TransitionModel
  obs_space: null
  action_space: null
  static_fns: null
  lr: null
  dynamics_lr: 0.001
  model_batch_size: 2048
  use_weight_decay: true
  optimizer_class: Adam
  holdout_ratio: 0.2
  inc_var_loss: true
  n_ensembles: 7
  n_elites: 5
  model:
    hidden_dims:
    - 200
    - 200
    - 200
    - 200
    decay_weights:
    - 2.5e-05
    - 5.0e-05
    - 7.5e-05
    - 7.5e-05
    - 0.0001
    act_fn: swish
    out_act_fn: identity
    num_elite: 5
    ensemble_size: 7
buffer:
  data:
    _target_: common.replay_buffer_wrapper.ReplayBufferWrapper
    buffer_size: 1000000
    obs_shape: 17
    action_dim: 6
    obs_dtype: float32
    action_dtype: float32
  model:
    _target_: common.replay_buffer_wrapper.ReplayBufferWrapper
    buffer_size: 1000000
    obs_shape: 17
    action_dim: 6
    obs_dtype: float32
    action_dtype: float32
rl_model:
  _target_: src.algos.mopo.MOPO
  reward_penalty_coef: 1.0
  mopo_params:
    max_epoch: 400
    rollout_batch_size: 50000
    rollout_mini_batch_size: 10000
    rollout_length: 5
    model_retain_epochs: 1
    num_env_steps_per_epoch: 1000
    train_model_interval: 250
    max_trajectory_length: 1000
    eval_interval: 1000
    num_eval_trajectories: 10
    snapshot_interval: 2000
    model_env_ratio: 0.95
    max_model_update_epochs_to_improve: 5
    max_model_train_iterations: null
policy:
  _target_: src.algos.sac.SACPolicy
  actor_lr: 0.0003
  critic_lr: 0.0003
  alpha_lr: 0.0003
  gamma: 0.99
  tau: 0.005
  alpha: []
  auto_alpha: true
  target_entropy: -3
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${hydra:run.dir}
  min_epochs: 1
  max_epochs: 10
  accelerator: cpu
  devices: 1
  check_val_every_n_epoch: 1
  deterministic: false
logger:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  save_dir: ${paths.output_dir}
  offline: false
  id: null
  anonymous: null
  project: ${algo_name}
  name: ''
  log_model: false
  prefix: ''
  group: ${env.task}
  tags: []
  job_type: ''
experiment:
  task: halfcheetah-random-v0
  transition_params:
    model_batch_size: 2048
    use_weight_decay: true
    optimizer_class: Adam
    learning_rate: 0.001
    holdout_ratio: 0.2
    inc_var_loss: true
    model:
      hidden_dims:
      - 200
      - 200
      - 200
      - 200
      decay_weights:
      - 2.5e-05
      - 5.0e-05
      - 7.5e-05
      - 7.5e-05
      - 0.0001
      act_fn: swish
      out_act_fn: identity
      num_elite: 5
      ensemble_size: 7
  mopo_params:
    max_epoch: 400
    rollout_batch_size: 50000
    rollout_mini_batch_size: 10000
    model_retain_epochs: 1
    num_env_steps_per_epoch: 1000
    train_model_interval: 250
    max_trajectory_length: 1000
    eval_interval: 1000
    num_eval_trajectories: 10
    snapshot_interval: 2000
    model_env_ratio: 0.95
    max_model_update_epochs_to_improve: 5
    max_model_train_iterations: None
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
algo_name: mbpo_uq
seed: 1
pretrained: true
mode: offline
policy_path: ''
model_path: saved_models
logdir: log
iter: 3
ckpt_path: saved_models
