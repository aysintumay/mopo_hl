## Overview

This is a modification to the MOPO algorithm with the integration of hydra lightning template.

## Dependencies

- MuJoCo 2.0
- Gym 0.22.0
- D4RL
- PyTorch 1.8+


## How It Works

All PyTorch Lightning modules are dynamically instantiated from module paths specified in config. Example model config:

```yaml
_target_: src.models.transition_model.TransitionModel
# Dynamics model training config
obs_space: null
action_space: null
static_fns: null
lr: 0.001
model_batch_size: 2048
use_weight_decay: True
optimizer_class: Adam
holdout_ratio: 0.2
inc_var_loss: True
# Model architecture config
n_ensembles: 7
n_elites: 5
reward_penalty_coef: 0.5
model:
  hidden_dims: [200, 200, 200, 200]
  decay_weights: [0.000025, 0.00005, 0.000075, 0.000075, 0.0001]
  act_fn: swish
  out_act_fn: identity
  num_elite: 5         
  ensemble_size: 7  
```

Using this config we can instantiate the object with the following line:

```python
model = hydra.utils.instantiate(config.model)
```

This allows you to easily iterate over new models! Every time you create a new one, just specify its module path and parameters in appropriate config file. <br>

Switch between models and datamodules with command line arguments:

```bash
python src/main_lightning.py model=ensemble-hopper rl_model=mopo-hopper
```

Example pipeline managing the instantiation logic: [src/main_lightning.py](src/main_lightning.py).

Override the main config parameters and modules [configs_ai/config.yaml](configs_ai/config.yaml) with the experiment configs in [configs_ai/experiment/](configs_ai/experiment/).


**Basic workflow**

1. Write your PyTorch Lightning module (see [models/mnist_module.py](src/models/mnist_module.py) for example)
2. Write your PyTorch Lightning datamodule (see [data/mnist_datamodule.py](src/data/mnist_datamodule.py) for example)
3. Write your experiment config, containing paths to model and datamodule
4. Run training with chosen experiment config: the following config overrides the model with world model config.
   ```bash
   python src/main_lightning.py experiment=wm_experiment.yaml
   ```
## Logs

Hydra creates new output directory for every executed run.

Default logging structure:

```
├── logs
│   ├── task_name
│   │   ├── runs                        # Logs generated by single runs
│   │   │   ├── YYYY-MM-DD_HH-MM-SS       # Datetime of the run
│   │   │   │   ├── .hydra                  # Hydra logs
│   │   │   │   ├── csv                     # Csv logs
│   │   │   │   ├── wandb                   # Weights&Biases logs
│   │   │   │   ├── checkpoints             # Training checkpoints
│   │   │   │   └── ...                     # Any other thing saved during training
│   │   │   └── ...
│   │   │
│   │   └── multiruns                   # Logs generated by multiruns
│   │       ├── YYYY-MM-DD_HH-MM-SS       # Datetime of the multirun
│   │       │   ├──1                        # Multirun job number
│   │       │   ├──2
│   │       │   └── ...
│   │       └── ...
│   │
│   └── debugs                          # Logs generated when debugging config is attached
│       └── ...
```

</details>

You can change this structure by modifying paths in [hydra configuration](configs/hydra).

- Some frequently used commands:

**Run MOPO with default transition model for hopper-expert-v0 dataset**
```
python src/main_lightning.py dataset=hopper-expert-v0 pretrained=True train.eval_episodes=1000 train.device=0
python src/main_lightning.py experiment=ensemble_hopper_expert_mopo

```
**Run MBPO with default transition model for hopper-expert-v0 dataset**
```
python src/main_lightning.py experiment=ensemble_hopper_expert_mbpo
```
- currently debugged 

**Run MOPO with default world model for hopper-expert-v0 dataset**
```
python src/main_lightning.py experiment=wm_hopper_expert_mopo
```
**Run MBPO with default world model for hopper-expert-v0 dataset**
```
python src/main_lightning.py experiment=wm_hopper_expert_mbpo
```





# Reference

- Official tensorflow implementation: [https://github.com/tianheyu927/mopo](https://github.com/tianheyu927/mopo)